---
title: "DATA 621: HW 2"
author: 'Group 2: Banu Boopalan, Gregg Maloy, Alexander Moyse, Umais Siddiqui'
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

**Instructions**        
In this homework assignment, you will work through various classification metrics. You will be asked
to create functions in R to carry out the various calculations. You will also investigate some
functions in packages that will let you obtain the equivalent results. Finally, you will create
graphical output that also can be used to evaluate the output of classification models, such as binary
logistic regression.

**1.  Download the classification output data set (attached in Blackboard to the assignment).**
```{r, warning=FALSE, message=FALSE}
library(readr)
library(dplyr)
library(tidyverse)
url <- "https://raw.githubusercontent.com/greggmaloy/621/main/classification-output-data.csv"
data <- read_csv(url)
print(data)
```
**2. The data set has three key columns we will use:**    

* class: the actual class for the observation      
* scored.class: the predicted class for the observation (based on a threshold of 0.5)        
* scored.probability: the predicted probability of success for the observation

**Use the table() function to get the raw confusion matrix for this scored dataset. Make sure you**
**understand the output. In particular, do the rows represent the actual or predicted class? The**
**columns?**

***Question 2 Answer:***     

Rows represent the predicted class.  The rows are essentially what is predicted by the model, with 0 representing no predicted condition/disease and 1 representing those individuals predicted by the model to have a condition/disease.

Columns represent the actual class.  The columns represent the true condition/disease state, with 0 representing the individual not having the condition/disease and 1 representing those individuals who actually have the condition/disease.

By arranging the actual and predicted classes in a confusion matrix/cross tabs/pivot tab, calculations ,ie sensitivity and specifity, can be derived which provide information about the models predictive power.

Below is a break down of the various sections of a confusion matrix:

Row 0, Column 0 (119): This represents True Negativesâ€”119 cases where the predicted class was 0 and the
actual class was also 0. In other words, there were 119 cases that were predicted to be negative that were actually negative (predicted negative and actually negative).    

Row 0, Column 1 (30): This represents False Negativesâ€”30 cases where the predicted class was 0 but the
actual class was 1.  In other words, there were 30 cases where the model does not predict the  condition/state, but that individual/case actually has the condition/disease.  (predicted negative and actually positive).       

Row 1, Column 0 (5): This represents False Positivesâ€”5 cases where the predicted class was 1 but the
actual class was 0. In other words, there were 5 cases where the model predicted the  condition/state/disease, but that individual/case actually did not have the condition/state/disease.  (predicted positive and actually negative)   

Row 1, Column 1 (27): This represents True Positivesâ€”27 cases where the predicted class was 1 and the
actual class was 1.  In other words, there were 27 case where both the prediction and actual state/condition/disease were positive. (predicted positive and actually positive)

```{r, warning=FALSE, message=FALSE}
confusion_matrix <- table(data$scored.class,data$class)
confusion_matrix
```
**3. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions.**    

In general, accuracy represents the percent of times a model correctly matches/identifies the actual state.  The accuracy of our model was determined to be 0.8066298. An accuracy of 0.8066298 indicates that the model is correct 80.66% of the time and incorrect the other ~19% of the time.  The formula is composed of the sum of true positives and true negatives divided by the sample size.
$$
\text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
$$

```{r, warning=FALSE, message=FALSE}
#function
calculate_accuracy <- function(data, actual_col, predicted_col) {
#TP,TN,FP, FN
  TP <- confusion_matrix[2, 2]  # Predicted 1, Actual 1
  TN <- confusion_matrix[1, 1]  # Predicted 0, Actual 0
  FP <- confusion_matrix[2, 1]  # Predicted 1, Actual 0
  FN <- confusion_matrix[1, 2]  # Predicted 0, Actual 1
  
#calculate accuracy
accuracy <- (TP + TN) / (TP + FP + TN + FN)
  return(accuracy)
}
#call function
accuracy <- calculate_accuracy(data, actual_col = "class", predicted_col = "scored.class")
print(accuracy)
```           
**4. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.  Verify that you get an accuracy and an error rate that sums to one.**


In general, the classification error rate represents the percent of times the model incorrectly matches/identifies the actual state. A classification error rate of 0.1933702 means that the model makes incorrect predictions approximately 19.34% of the time. The formula is composed of the sum of false positives and false negatives divided by the sample size.  

Additionally, the classification error rate added to the accuracy will always equal 1 because when added together, this is because the classification error and the precision rate combined, account for every observation in the dataset.

$$
\text{Classification Error Rate} = \frac{FP + FN}{TP + FP + TN + FN}
$$

```{r, warning=FALSE, message=FALSE}
calculate_classification_error <- function(data, actual_col, predicted_col) {
  TP <- confusion_matrix[2, 2]  # Predicted 1, Actual 1
  TN <- confusion_matrix[1, 1]  # Predicted 0, Actual 0
  FP <- confusion_matrix[2, 1]  # Predicted 1, Actual 0
  FN <- confusion_matrix[1, 2]  # Predicted 0, Actual 1
  
classification_error <- (FP + FN) / (TP + FP + TN + FN)
  return(classification_error)
}
#call function
classification_error <- calculate_classification_error(data, actual_col = "class", predicted_col = "scored.class")

#result
print(classification_error)
validate<-classification_error +accuracy
print(paste("Classification error + accuracy =", validate))
            
```           
**5. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified and returns the precision of the predictions.**     


Precision measures the proportion of true positive predictions among all positive predictions. Our model's precision of 0.84375 means that 84.38% of the instances predicted as positive by the model were correctly identified as true positives.  In other words, precision measures the amount of the population/sample which are correctly predicted to be positive divided by all of those who actually have the state/condition/disease, including false positives.
$$
\text{Precision} = \frac{TP}{TP + FP}
$$ 

```{r, warning=FALSE, message=FALSE}
calculate_precision <- function(data, actual_col, predicted_col) {
  confusion_matrix <- table(data[[predicted_col]], data[[actual_col]])
  TP <- confusion_matrix[2, 2]  # Predicted 1, Actual 1
  FP <- confusion_matrix[2, 1]  # Predicted 1, Actual 0
  precision <- TP / (TP + FP)
  return(precision)
}
precision <- calculate_precision(data, actual_col = "class", predicted_col = "scored.class")
print(paste("Precision: ", precision))
```
**6. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall.**       
Sensitivity measures the proportion of actual positive instances that are correctly identified by the model. The sensitivity for our model was determined to be 0.4737. A sensitivity of this amount can be interpreted as the model correctly identifies ~47% of the actual positive cases.  In other words, the model was unable to predict about half of the actually positive cases.

$$
\text{Sensitivity} = \frac{TP}{TP + FN}
$$
```{r, warning=FALSE, message=FALSE}
calculate_sensitivity <- function(data, actual_col, predicted_col) {
  confusion_matrix <- table(data[[predicted_col]], data[[actual_col]])
  TP <- confusion_matrix[2, 2]  # Predicted 1, Actual 1
  FN <- confusion_matrix[1, 2]  # Predicted 0, Actual 1
  sensitivity <- TP / (TP + FN)
  return(sensitivity)
}


sensitivity <- calculate_sensitivity(data, actual_col = "class", predicted_col = "scored.class")
print(paste("Sensitivity:", sensitivity))
```
**7. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions.**    

Specificity measures the proportion of actual negative instances that are correctly identified by the model. The specificity of our mode was determined to be 0.9597.  A specificity of this amount means that the model correctly identifies ~96% of the actual negative cases.
$$
\text{Specificity} = \frac{TN}{TN + FP}
$$

```{r, warning=FALSE, message=FALSE}
calculate_specificity <- function(data, actual_col, predicted_col) {
  confusion_matrix <- table(data[[predicted_col]], data[[actual_col]])
  TN <- confusion_matrix[1, 1]  # Predicted 0, Actual 0
  FP <- confusion_matrix[2, 1]  # Predicted 1, Actual 0
  specificity <- TN / (TN + FP)
  return(specificity)
}
specificity <- calculate_specificity(data, actual_col = "class", predicted_col = "scored.class")
print(paste("Specificity:", specificity))
```    
**8. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,and returns the F1 score of the predictions.**    
The F1 score is essentially a tool/ metric utilized to balance precision and sensitivity, as in some cases, precision and sensitivity are a trade off, with one being more desirable than the other. Our model was determined to have an F1 score of 0.606741573033708. An F1 score of 60.67% represents a moderate F1 score with room for improvement.
$$
\text{F1 Score} = \frac{2 Ã— Precision Ã— Sensitivity}{Precision + Sensitivity}
$$
```{r, warning=FALSE, message=FALSE}
calculate_f1_score <- function(data, actual_col, predicted_col) {
  precision   <- calculate_precision(data, actual_col, predicted_col)
  sensitivity <- calculate_sensitivity(data, actual_col, predicted_col)
  f1_score    <- 2 * (precision * sensitivity) / (precision + sensitivity)
  return(f1_score)
}
f1_score <- calculate_f1_score(data, actual_col = "class", predicted_col = "scored.class")
print(paste("F1 Score:", f1_score))
```
**9. Before we move on, letâ€™s consider a question that was asked: What are the bounds on the F1 score? Show that the F1 score will always be between 0 and 1. (Hint: If 0 < ð‘Ž < 1 and 0 < ð‘ < 1 then ð‘Žð‘ < ð‘Ž.)**

The bounds are between 0 and 1 since the F1 score is a ratio.

Below is the formula for the F1 score
$$
\text{F1 Score} = \frac{2 Ã— Precision Ã— Sensitivity}{Precision + Sensitivity}
$$
We know that highest possible value for both precision and sensitivity individually is 0.99 because precision and sensitivity are ratios. (If 0 < precision < 1 and 0 < sensitivity < 1 then ð‘Žð‘ < ð‘Ž.)

We can demonstrate this by plugging in the highest possible values for precision and sensitivity into the F1 equation.

$$
\text{F1 Score} = \frac{2 Ã— 0.99 Ã— 0.99}{0.99 + 0.99}
$$
$$
\text{F1 Score} = 0.99 = \frac{1.9602}{1.98}
$$    
Likewise we can plug the lowest value (0.01) into the F1 Score formula.
$$
\text{F1 Score} = 0.01 = \frac{0.0002}{0.02}
$$ 
```{r, warning=FALSE, message=FALSE}
```
**10. Write a function that generates an ROC curve from a data set with a true classification column (class in our example) and a probability column (scored.probability in our example). Your function should return a list that includes the plot of the ROC curve and a vector that contains the calculated area under the curve (AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals.**    
```{r, warning=FALSE, message=FALSE}
roc_func <- function(x, y) {
  thresholds <- sort(unique(y), decreasing = TRUE)
  TP <- numeric(length(thresholds))
  FP <- numeric(length(thresholds))
  
  for (i in seq_along(thresholds)) {
    TP[i] <- sum(x[y >= thresholds[i]]) / sum(x) 
    FP[i] <- sum(!x[y >= thresholds[i]]) / sum(!x) 
  }
  
  auc <- sum(diff(FP) * (TP[-1] + TP[-length(TP)]) / 2)
  
  df <- data.frame(TP = TP, FP = FP)
  return(list(df = df, auc = auc))
}

roc_data <- roc_func(data$class, data$scored.probability)


plot(roc_data$df$FP, 
     roc_data$df$TP, 
     type = "l", 
     col = "blue", 
     lwd = 2,
     xlab = "Specificity",
     ylab = "Sensitivity",
     main = paste("ROC Curve (AUC =", round(roc_data$auc, 3), ")"))
abline(a = 0, b = 1, col = "red", lty = 2) # Diagonal line
```

```{r, warning=FALSE, message=FALSE}
library(knitr)
kable(roc_data$auc)
```

**11. Use your created R functions and the provided classification output data set to produce all of the classification metrics discussed above.**     
```{r, warning=FALSE, message=FALSE}
df <- c(accuracy, classification_error, f1_score, precision, sensitivity, specificity)
names(df) <- c("Accuracy", "Error", "F1", "Percision", "Sensitivity", "Spec")
kable(df)
```
    
**12. Investigate the caret package. In particular, consider the functions confusion Matrix, sensitivity, and specificity. Apply the functions to the data set. How do the results compare with your own functions?**      

The caret results are identical to the results of our functions. Sensitivity is identical (caret=0.4737 vs our function=0.4737), specificity is identical (caret=0.9597 vs our function=0.9597), accuracy is identical (caret=0.8066 vs our function=0.8066), and precision is identical (caret=0.8438 vs our function=0.8438).  The caret package did not caluclate F1 score, but if it did, it would be identical.
```{r, warning=FALSE, message=FALSE}
#install.packages("caret")
#install.packages("proc")
```

```{r, warning=FALSE, message=FALSE}
library(caret)

data$class <- factor(data$class, levels = c(1, 0))  
data$scored.class <- factor(data$scored.class, levels = c(1, 0))  

conf_matrix <- confusionMatrix(data$scored.class, data$class)

print(conf_matrix)

#sensitivity
sens <- sensitivity(data$scored.class, data$class)
print(paste("Sensitivity (Recall):", sens))

#specificity
spec <- specificity(data$scored.class, data$class)
print(paste("Specificity:", spec))

```
**13. Investigate the pROC package. Use it to generate an ROC curve for the data set. How do the results compare with your own functions?**    

Both graphs are visually comparable, and both the pROC package and my custom function yielded an identical AUC of 0.85. This AUC value indicates a strong model fit, demonstrating that the model effectively identifies true positives while minimizing false positives. 
```{r, warning=FALSE, message=FALSE}

library(pROC)
roc_obj <- roc(data$class, data$scored.probability)
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve using pROC", legacy.axes = TRUE)
auc_value <- auc(roc_obj)
print(paste("AUC from pROC:", auc_value))
```